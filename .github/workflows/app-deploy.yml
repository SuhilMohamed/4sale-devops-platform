name: Application Pipeline

on:
  push:
    branches: [ main ]
    paths:
      - 'app/**'
      - 'k8s/**'
      - 'docker-compose.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'app/**'
      - 'k8s/**'
      - 'docker-compose.yml'

env:
  REGISTRY: ghcr.io
  IMAGE_NAME_BACKEND: ${{ github.repository }}/task-backend
  IMAGE_NAME_FRONTEND: ${{ github.repository }}/task-frontend

jobs:
  code-quality:
    name: Code Quality & Testing
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: app/backend/package-lock.json

      - name: Install backend dependencies
        run: |
          cd app/backend
          npm ci

      - name: Run ESLint
        run: |
          cd app/backend
          npm run lint || true

      - name: Run Prettier
        run: |
          cd app/backend
          npm run format:check || true

      - name: Run unit tests
        run: |
          cd app/backend
          npm test

      - name: Generate test coverage
        run: |
          cd app/backend
          npm run test:coverage || true

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./app/backend/coverage/lcov.info
          fail_ci_if_error: false

  security-scanning:
    name: Security Scanning
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run CodeQL Analysis
        uses: github/codeql-action/init@v2
        with:
          languages: javascript

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v2

      - name: Run Snyk to check for vulnerabilities
        uses: snyk/actions/node@master
        env:
          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
        with:
          args: --file=app/backend/package.json --severity-threshold=high
        continue-on-error: true

      - name: Upload Snyk results to GitHub Code Scanning
        uses: github/codeql-action/upload-sarif@v2
        if: always()
        with:
          sarif_file: snyk.sarif
        continue-on-error: true

  build-and-test:
    name: Build & Integration Tests
    runs-on: ubuntu-latest
    needs: [code-quality]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata (tags, labels) for backend
        id: meta-backend
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME_BACKEND }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-

      - name: Extract metadata (tags, labels) for frontend
        id: meta-frontend
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME_FRONTEND }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-

      - name: Build backend image
        uses: docker/build-push-action@v5
        with:
          context: ./app/backend
          platforms: linux/amd64
          push: true
          tags: ${{ steps.meta-backend.outputs.tags }}
          labels: ${{ steps.meta-backend.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build frontend image
        uses: docker/build-push-action@v5
        with:
          context: ./app/frontend
          platforms: linux/amd64
          push: true
          tags: ${{ steps.meta-frontend.outputs.tags }}
          labels: ${{ steps.meta-frontend.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Run Trivy vulnerability scanner on backend
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME_BACKEND }}:${{ github.sha }}
          format: 'sarif'
          output: 'backend-trivy-results.sarif'
        continue-on-error: true

      - name: Run Trivy vulnerability scanner on frontend
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME_FRONTEND }}:${{ github.sha }}
          format: 'sarif'
          output: 'frontend-trivy-results.sarif'
        continue-on-error: true

      - name: Upload Trivy scan results to GitHub Security tab
        uses: github/codeql-action/upload-sarif@v2
        if: always()
        with:
          sarif_file: 'backend-trivy-results.sarif'
        continue-on-error: true

      - name: Start integration test environment
        run: |
          docker-compose up -d
          sleep 30

      - name: Run integration tests
        run: |
          # Wait for services to be ready
          timeout 60 bash -c 'until curl -f http://localhost:3000/health; do sleep 2; done'
          
          # Run integration tests
          cd app/backend
          npm run test:integration || true

      - name: Run API tests
        run: |
          # Test API endpoints
          curl -f http://localhost:3000/health
          curl -f http://localhost:3000/listTasks
          
          # Test frontend
          curl -f http://localhost:8080

      - name: Stop integration test environment
        if: always()
        run: docker-compose down

  security-hardening:
    name: Security Hardening
    runs-on: ubuntu-latest
    needs: [build-and-test, security-scanning]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install Cosign
        uses: sigstore/cosign-installer@v3
        with:
          cosign-release: 'v2.2.0'

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Sign backend container image
        run: |
          cosign sign --yes ${{ env.REGISTRY }}/${{ env.IMAGE_NAME_BACKEND }}:${{ github.sha }}

      - name: Sign frontend container image
        run: |
          cosign sign --yes ${{ env.REGISTRY }}/${{ env.IMAGE_NAME_FRONTEND }}:${{ github.sha }}

      - name: Generate SBOM for backend
        uses: anchore/sbom-action@v0
        with:
          image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME_BACKEND }}:${{ github.sha }}
          format: spdx-json
          output-file: backend-sbom.spdx.json

      - name: Generate SBOM for frontend
        uses: anchore/sbom-action@v0
        with:
          image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME_FRONTEND }}:${{ github.sha }}
          format: spdx-json
          output-file: frontend-sbom.spdx.json

      - name: Upload SBOM artifacts
        uses: actions/upload-artifact@v3
        with:
          name: sbom-reports
          path: "*-sbom.spdx.json"

  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [security-hardening]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    environment: staging
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-west-2

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.2'

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region us-west-2 --name 4sale-devops-staging

      - name: Validate Kubernetes manifests
        run: |
          kubectl apply --dry-run=client -f k8s/base/
          kubectl apply --dry-run=client -f k8s/overlays/staging/

      - name: Update image tags in manifests
        run: |
          sed -i "s|task-backend:.*|${{ env.REGISTRY }}/${{ env.IMAGE_NAME_BACKEND }}:${{ github.sha }}|g" k8s/base/backend-deployment.yaml
          sed -i "s|task-frontend:.*|${{ env.REGISTRY }}/${{ env.IMAGE_NAME_FRONTEND }}:${{ github.sha }}|g" k8s/base/frontend-deployment.yaml

      - name: Deploy to staging
        run: |
          kubectl apply -f k8s/base/namespace.yaml
          kubectl apply -f k8s/base/ -n task-app-staging
          kubectl apply -f k8s/overlays/staging/ -n task-app-staging

      - name: Wait for deployment
        run: |
          kubectl rollout status deployment/task-backend -n task-app-staging --timeout=300s
          kubectl rollout status deployment/task-frontend -n task-app-staging --timeout=300s

      - name: Run health checks
        run: |
          # Get the load balancer URL
          BACKEND_URL=$(kubectl get service task-backend-service -n task-app-staging -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          FRONTEND_URL=$(kubectl get service task-frontend-service -n task-app-staging -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          
          # Wait for load balancer to be ready
          sleep 60
          
          # Health checks
          curl -f http://$BACKEND_URL/health || echo "Backend health check failed"
          curl -f http://$FRONTEND_URL || echo "Frontend health check failed"

  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [deploy-staging]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    environment: production
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-west-2

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.2'

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region us-west-2 --name 4sale-devops-production

      - name: Update image tags in manifests
        run: |
          sed -i "s|task-backend:.*|${{ env.REGISTRY }}/${{ env.IMAGE_NAME_BACKEND }}:${{ github.sha }}|g" k8s/base/backend-deployment.yaml
          sed -i "s|task-frontend:.*|${{ env.REGISTRY }}/${{ env.IMAGE_NAME_FRONTEND }}:${{ github.sha }}|g" k8s/base/frontend-deployment.yaml

      - name: Deploy to production
        run: |
          kubectl apply -f k8s/base/namespace.yaml
          kubectl apply -f k8s/base/ -n task-app
          kubectl apply -f k8s/overlays/production/ -n task-app

      - name: Wait for deployment with zero downtime
        run: |
          kubectl rollout status deployment/task-backend -n task-app --timeout=600s
          kubectl rollout status deployment/task-frontend -n task-app --timeout=600s

      - name: Post-deployment health checks
        run: |
          # Get the production load balancer URL
          BACKEND_URL=$(kubectl get service task-backend-service -n task-app -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          FRONTEND_URL=$(kubectl get service task-frontend-service -n task-app -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          
          # Wait for load balancer to be ready
          sleep 60
          
          # Comprehensive health checks
          curl -f http://$BACKEND_URL/health
          curl -f http://$BACKEND_URL/metrics
          curl -f http://$FRONTEND_URL

      - name: Notify deployment success
        if: success()
        run: |
          echo "✅ Production deployment successful!"
          echo "Backend: http://$(kubectl get service task-backend-service -n task-app -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')"
          echo "Frontend: http://$(kubectl get service task-frontend-service -n task-app -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')"
